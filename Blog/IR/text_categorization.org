# -*- mode: org -*-
# Last modified: <2012-05-23 17:16:02 Wednesday by richard>
#+STARTUP: showall
#+TITLE:   Text Categorization

* 定义
  面向文本分类任务则成为文本分类或主题发现。

* 目标
  文本分类的目标是在测试数据或新数据中获得高精确率的结果。


  1. 什么是文本分类。
     为了sldflsdjflsdkj。

  2. 为什么要进行文本分类。
     在自然语言处理中，进行文本分类可以更好的区分各类信息，有效提高其
     他自然语言处理项目的准确率。比如分词等内容。
  3. 文本分类主要有哪几个步骤。
     预处理，文本表示，分类器。
  4. 为什么要采用这些步骤。
     1. 采用预处理将文本分割成词，组成一个由词表组成的向量。以便进行下一步的处理。
     2. 由于文本分成词将会是非常高维，很难区分哪个词项的有效性更高，通
        过特征选取方案选取更能够体现区分能力的词语。可以降低维数并提高
        准确率。
        然后通过特征权重算法来计算出各种特征项对分类的具体效用。
     3. 通过向量机等方法来进行快速的分类。

  5. 采取这些步骤的不同方法的对比，孰优孰劣。
* 学习过程。

* 分类应用
  1. 垃圾网页的自动判定。
  2. 色情内容判定。
  3. 情感发现。
  4. 个人邮件组织和整理。
  5. 面向主题的搜索。垂直搜索。
  6. 基于文档分类器来构建。


* 向量空间模型（VSM）
  支持向量机方法是建立在统计学习理论的VC 维理论和结构风险最小原理基础
  上的。
  根据有限的样本信息在模型的复杂性（即对特定训练样本的学习精度，
  Accuracy）和学习能力（即无错误地识别任意样本的能力）之间寻求最佳折衷，
  以期获得最好的推广能力[14]（或称泛化能力）。


  SVM解决问题的时候，和样本的维数是无关的（甚至样本是上万维的都可以，这
  使得SVM很适合用来解决文本分类的问题，当然，有这样的能力也因为引入了核
  函数）
** 假设
   同一类文档会构成一个邻近区域，而不同类的邻近区域之间是互不重叠的。
   文档集是否会映射成邻近取决于在文档中表示的许多选项。


   使用分类器在样本数据上的分类的结果与真实结果（因为样本是已经标注过
   的数据，是准确的数据）之间的差值来表示。
** 小样本
   并不是说样本的绝对数量少（实际上，对任何算法来说，更多的样
   本几乎总是能带来更好的效果），而是说与问题的复杂度比起来，SVM算法要
   求的样本数是相对比较少的。
** 非线性，
   是指SVM擅长应付样本数据线性不可分的情况，主要通过松弛变量（也有人叫
   惩罚变量）和核函数技术来实现，这一部分是SVM的精髓.

** 除了VSM文本表示方法以外，还有另外一些表示方法：
  - 词组表示法
    但这些表示方法对文本分类效果的提高并不明显。原因可能在于，词组虽然提
    高了特征向量的语义含量，但却降低了特征向量的统计质量，使得特征向量变
    得更加稀疏，让机器学习算法难以从中提取用于分类的统计特性。
  - 概念表示法
    用概念作为特征向量的特征表示。用概念表示法的时候需要额外的语言资源，
    主要是一些语义词典。用概念代替单个词可以在一定程度上解决自然语言的
    歧义性和多样性给特征向量带来的噪声问题，有利于提高文本分类的效果。

* 文本特征选取方法：

** 基于文档频率DF的特征提取方法。
   文档频率DF是指出现某个特征项的文档的频率。

*** 选取方法
    当该特征项DF值小于某个阈值的时候，从特征空间中去掉该特征，因为该特
    征项使文档出现的频率太低，没有代表性；
    当该特征项DF值大于某个阈值的时候，从特征空间中去掉该特征，因为该特
    征项使文档出现的频率太高，没有区分度。

*** pros and cons
    基于文档频率的特征选择方法可以降低向量计算的复杂度，并可能提高分类
    的准确率。因为按这种选择方法可以去掉一部分噪声特征。
    简单易行。
    事实上频率低的特征往往包含较多的信息，对于分类的重要性很大，对于这
    类特性就不应该使用DF方法将其直接排除在向量特征之外。

** 信息增益法。IG
   IG依据某特征项t_i为整个分类所能提供的信息量的多少来衡量该特征项的重
   要程度，从而决定对该特征项的取舍。
   某个特征项t_i的信息增益是指有该特征或没有该特征时，为整个分类所能提
   供的信息量的差别，其中信息量的多少由熵来衡量。因此，信息增益即不考
   虑任何特征时文档的熵和考虑该特征文档的熵的差值。

*** pros and cons
    从信息增益的定义可知，一个特征的信息增益实际上描述的是它包含的能够
    帮助预测类别属性的信息量。从理论上讲，信息增益应该是最好的特征选取方法，但实际上由于许多信息增益比较高的特征往往出现频率较低，往往会存在数据稀疏的问题，此时分类效果也较差。
** X^2统计量(CHI)法

** 互信息(MI)方法。

** DTP

** 期望交叉熵法。

** 文本证据权法。

** 优势率方法。

** 组合特征提取方法。

** 强类信息词。

* 分类器

** 朴素贝叶斯分类器
   朴素贝叶斯的性能并不理想，原因可能是由于这种分类器的性能容易受分类
   任务的影响

** 基于支持向量机的分类器
   主要用于解决2元模式分类问题。

** k-最邻近法kNN

** 神经网络法
   效果很差

** 线性最小平方拟合法LLSF
   LLSF方法稍逊色于kNN和SVM方法

** 决策树分类法
   信息增益是决策树训练中常用的衡量给定属性区分寻量样本能力的定量标准。

** 模糊分类法

** Rocchio分类法
   计算简单易行，分类效果仅次于kNN方法和SVM方法。

** 基于投票的分类方法
   * Boosting算法
     最流行的是AdaBoost方法。该方法在文本分类领域中有着非常广泛的应用。
   * Bagging算法

* 文本分类器的性能评估方法
** 正确率，召回率和F-测度值
   假设一个文本分类器输出的统计结果如下所示：
   | 分类器判断结果\实际关系   | 属于         | 不属于       |
   | <25>                      | <12>         | <12>         |
   |---------------------------+--------------+--------------|
   | 标记为 YES                | a            | b            |
   | 标记为 NO                 | c            | d            |
   召回率 r = a/(a+c) * 100%
   正确率 p = a/(a+b) * 100%
   F-测度值 F_b = (B^2 + 1) * p * r / (B ^ 2 * p + r)
   通常取B = 1, 此时F-测度值为：
   F_b = (2 * p * r)/(p + r)
** 微平均召回率
   微平均分类是利用被正确分类的总的正确率和召回率，然后替换上文中a,b,c。然后得到最终的召回率

** 宏平均召回率
   则是先计算出所有的召回率和正确率，最后在平均结果。

** 文本分类任务
