# -*- mode: org -*-
# Last modified: <2013-01-29 19:09:28 Tuesday by richard>
#+STARTUP: showall
#+LaTeX_CLASS: chinese-export
#+TODO: TODO(t) UNDERGOING(u) | DONE(d) CANCELED(c)
#+TITLE:   Conclusion of Stopword
#+AUTHOR: Richard Wong

本文总结了我所认知的停用词的领域知识和一些自动化的识别方法。

* basic idea
  something we need in the preprocessing stage.
** 确定停用词的识别范围：(charset)
   4e00-9fff范围的unicode字符。 CJK 基本段，超过的都可以strip掉。
   (normally speaking)
   其他语言（如英文）需要额外处理。
   |-----------------------------------------+-------------+------------------------------------------------------|
   | Block                                   | Range       | Comment                                              |
   |-----------------------------------------+-------------+------------------------------------------------------|
   | CJK Unified Ideographs                  | 4E00-9FFF   | Common                                               |
   | CJK Unified Ideographs Extension A      | 3400-4DFF   | Rare                                                 |
   | CJK Unified Ideographs Extension B      | 20000-2A6DF | Rare, historic                                       |
   | CJK Compatibility Ideographs            | F900-FAFF   | Duplicates, unifiable variants, corporate characters |
   | CJK Compatibility Ideographs Supplement | 2F800-2FA1F | Unifiable variants                                   |
   |-----------------------------------------+-------------+------------------------------------------------------|

** 停用词和分词算法
   停用词非常依赖分词算法的特征（优劣）。不同的分词算法所识别的词语应
   该使用的停用词应该不同。

** 标点。
   1. 中英文标点统一非常重要。(re.U 很方便：）)
   2. 标点在不同的场合作用也是非常明显的，并不能简单的当作停用词去取出。
      标点可以表述性格，甚至完全反义一句话。
*** 需要斟酌的几个标点。
    #+begin_src unicode
    <>《》？！+-
    #+end_src

*** 无所谓的几个标点。
    #+begin_src unicode
    。@￥……&^*（）`=[]""|\;:,./~_﹏﹏—.‧
    #+end_src

** 比较常用的停用词方案应该是：
   类似于将基本的停用词分为3类：
*** 首先自动识别停用词：（见最后）
    1. 如果是分类问题：（判断类属关系，这个表现非常好）
       首先通过互信息（MI）或者信息增益（IG）等算法来自动识别停用词。
    2. 如果不是：（我做过，效果较好，但是需要人工干预）
       采用平均(tf-)idf 等算法来进行。
*** 然后通过领域知识排除。（跟预处理并不是一个过程）
    需要先讨论领域知识，根据特定的方法进行自动排除。（理想化）
*** 维护的手工列表。
    手工处理需谨慎，一旦手工，你很难回头改进自动识别算法。而且很容易加
    入偏见的东西，从而造成结果不准确。
    尽量讲手工列表处理为领域知识自动化的内容。（更易修改，可维护性好）


* Domain Knowledge for stopword.
** Common Knowledge
   词主要分为：
   实词：名词、动词、形容词、数词、量词和代词。
   虚词：副词、介词、连词、助词、拟声词和叹词

*** 实词
**** 名词
     主要用来实体识别，一般都不会认为是停用词。
     当然惯用语如狗屎之类语气词的很容易被当成停用词。（我想这个在
     retail里面还是挺有用的）
**** 动词
     一般来讲很多动词都不会作为停用词，而且是非常关键的地方。
     但是无语义的动词也很多，但基本上都可以通过最基本的自动停用词算法
     识别出来。

**** 形容词
     word contains '的'.
     形容词一般

**** 数词
     这个一般都会认为无意义然后丢掉。
     contains
     1. 基数
     2. 序数
     3. 约数
     4. 倍数
     5. 分数
     但是有些有意义的词如“二百五”“三七”需要额外提取出来。(！有意义词)

**** 量词
     量词一般都是停用词范畴。
     #+begin_src unicode
     一(x) eg:"一辆" "一部"
     二(x)
     三(x)
     etc.
     #+end_src

**** 代词
     代词如果涉及语义的话，往往也

*** 虚词
    虚词这里在处理性格和写作倾向等潜关系的时候，往往被认为是一个更重要
    的feature。
    尤其在判断人物性格，乃至文章抄袭和年龄性别的时候。
    但是如果在nlp中变换成bag-of-word的时候基本无法判断具体语素，如果没
    有词性标注，则基本上都可以视为停用词。
**** 副词
     contains '地'.
     定义中说道有提题时所用的词: "如何"、"谁"、"何时"、"什么"

     有的时候副词可能会和动词被分词算法分成一个词。（如：“不”）可能
     就分出来“不是”这个词。所以有时副词非常有意义，需要谨慎对待。

**** 介词
     words list:
     #+begin_src unicode
     了, 把, 和, 于, 向, 依, 让, 在, 被, 由, 为, 因, 从, 当, 比, 给,
     与, 靠, 以, 除, 照, 自, 关, 用, 对, 拿
     #+end_src

**** 连词
     这些一般不表示具体的含义，但是语气分析和教育程度（有人更倾向于用
     更逻辑的句子）时候是非常重要的feature。
     words list:
     和,跟,与,既,及,而,或,或者,还是,但是,不过,虽然,然而,因为,因此,所以,
     由于,不但,不仅,而且,何况,并,且,不管,只要,除非,如果,即使,假若,以,
     以便,以免,为了,便,于是,然后

**** 拟声词
     是摹拟事物的声音的一种词汇。在汉语里，它只是汉字当成“音标”符号，
     用来表音，而和字义无关。
     一般来讲，停用词是都需要除去拟声词的。
     但是形容人的感叹的声音的词如“啊呀”、“唉”不算是象声词。
     * unicode =53e8 - 53d6= 序列的都是unicode 口旁的。看stopword的那个
       列表，可以regexp出来，我也没有全部总结了。


**** 助词
     这列表我基本上都当停用词干掉了。
     #+begin_src python
     者, 的, 了, 焉, 而, 与, 云, 尔, 吗, 或, 嘛, 欤, 吧, 邪, 耳, 耶,
     为, 呀, 哇, 哉, 么, 啊, 乎, 呢, 也, 矣, 罢, 啦, 哪, 兮, 已, 然
     #+end_src

**** 叹词
     现在网络语言各种奇葩拟声词都有，需要斟酌叹词范围。
     一般会去拟声词里面找。前面的机器方法可能会帮你查找某些奇葩叹词出来。
     words contains:
     #+begin_src unicode
     "呵", "呸", "呼"
     #+end_src



*** 其他奇葩字符。(但是在分类年龄时候可能需要额外考虑，比如火星文。。。)
    看起来量不是很大的都可以kill掉。。基本上strip掉就可以。机器判别对
    这个的处理非常好。
    量大了建立一个映射表也很重要。火星文映射表。
    1. u'丶'。

*** 繁体字。
    如果仅涉及语义处理的时候：
    wikipedia有一套繁体字转换集，很全面，可以使用。但是我感觉用繁体字
    的少于5% 处理他也用处不大，但是有的时候确实容易让分类非常偏置。
    （毕竟训练样本会很少）
    但我考虑在retail时候专门有各地买家，应该训练样本很足，可以得到一些
    更为优良的结果。
    暂时对于繁体字采取：1. 转成简体，2. 丢掉，没有很好的处理方式（太偏
    置）。

* manual.
  手工列表没啥说的。积累是一个比较重要的因素。（这三个看起来有点互斥的，
  但都可以很有效的提高精度。）

* PS:
  1. 停用词需要不停积累出来。附属的列表是共用表，其他的一般都是根据具
     体的领域知识处理的。

** PS+
   附送我之前的停用词的笔记。在选择自动处理停用词时候可能需要用到的

* 自动停用词方案主要有：[1]
1. 文档频数（ DF）
   DF是一种简单的评估函数，其值为训练集合中包含次单词的文本数，DF评估函数的理论假设是当一个词在大量文本中出现时,这个词通常被认为是噪声词。
2. 词频（WF）
   WF同样是一种简单的评估函数，其值为训练集合中次单词发生的词频数。WF
   评估函数的理论假设是当一个词在大量出现时，通常被人为是噪声词。
3. 熵计算 ( entropy calculation, EC)[3]
   熵计算是一个基于单词出现的平均信息量，对词的有效性进行计算。
   W(w)=1+1/(ln(n))\Sigma{p_i(w)ln[p_i(w)]}
4. 联合熵 ( union entropy, UE)
   基于词在句子中出现的频率与该词包含个icid句子频率的联合熵分别计算词
   条在语料库中各个句子内发生的概率，以及包含该词条的句子在文本中发生
   的概率P_j，计算他们的熵，并根据他们的联合熵选取停用词：
   当一个词在句子中出现的平均信息量和包含该词的句子的平均信息量较大时,
   表示该词较为普通.应用该方法可以有效避免语料选取不均衡造成的停用词选
   取错误.

** 自动停用词方案的选择标准
   如果对停用词按照其出现的文本频数降序排序,用前10个停用词削减特征向量
   空间,不会产生负面影响;用前100个停用词削减特征向量空间,所产生的负面
   影响非常小.[2]
   其中基于熵计算的选取方式更倾向于选取文本中稳定出现的词，因此更容易
   受到文本行文方式等的影响。用联合熵法选取的停用词更倾向于选取在句子
   中稳定出现且出现较多的词，因此受文本的行文方式影响较少，选取出的停
   用词更能反映文本的真实情况。[1]
   而微博是一个更容易出现各类行文的场所，所以联合熵法将会更适用于微博
   的使用。粒度选取为句。可能需要通过句来处理问题。


[1]:顾益军,樊孝忠,王建华,等.中文停用词表的自动选取[J].北京理工大学学报,2005,25(04):337—340.
[2]:Sinka M P, Corne D W. Web intelligence WI 2003
Proceedings IEEE/ WIC International Conf erence on Soc[C]. Los Alamitos, IEEE Comput ,
2003. 396-402.
[3]:Yang Y. Pedersen J O. A comparative study on
feature selection in text categorization[A].
ICML-97, 14th International
Morgan Kauf mann Publishers Inc. , 1997. 412-420.
