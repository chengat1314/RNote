# -*- mode: org -*-
# Last modified: <2013-04-13 06:27:25 Saturday by richard>
#+STARTUP: showall
#+LaTeX_CLASS: chinese-export
#+TODO: TODO(t) UNDERGOING(u) | DONE(d) CANCELED(c)
#+TITLE:   Machine Learning syllabus
#+AUTHOR: Richard Wong

Skip Navigation This page features MathJax technology to render mathematical formulae. If you are using a screen reader, please visit MathPlayer to download the plugin for your browser. Please note that this is an Internet Explorer-only plugin at this time. Machine Learning
Top Navigation BarCoursesAbout


Machine Learning
Andrew Ng
Professor of Computer Science
Video Lectures

Having trouble viewing lectures? Try changing your player in course preferences.

* DONE I. Introduction (Week 1)
  Welcome (7 min)
  What is Machine Learning? (7 min)
  Supervised Learning (12 min)
  Unsupervised Learning (14 min)
* DONE II. Linear Regression with One Variable (Week 1)
  Model Representation (8 min)
  Cost Function (8 min)
  Cost Function - Intuition I (11 min)
  Cost Function - Intuition II (9 min)
  Gradient Descent (11 min)
  Gradient Descent Intuition (12 min)
  Gradient Descent For Linear Regression (10 min)
  What's Next (6 min)
* UNDERGOING III. Linear Algebra Review (Week 1, Optional)
  Matrices and Vectors (9 min)
  Addition and Scalar Multiplication (7 min)
  Matrix Vector Multiplication (14 min)
  Matrix Matrix Multiplication (11 min)
  Matrix Multiplication Properties (9 min)
  Inverse and Transpose (11 min)
* DONE IV. Linear Regression with Multiple Variables (Week 2)
  Multiple Features (8 min)
  Gradient Descent for Multiple Variables (5 min)
  Gradient Descent in Practice I - Feature Scaling (9 min)
  Gradient Descent in Practice II - Learning Rate (9 min)
  Features and Polynomial Regression (8 min)
  Normal Equation (16 min)
  Normal Equation Noninvertibility (Optional) (6 min)
* TODO V. Octave Tutorial (Week 2)
  Basic Operations (14 min)
  Moving Data Around (16 min)
  Computing on Data (13 min)
  Plotting Data (10 min)
  Control Statements: for, while, if statements (13 min)
  Vectorization (14 min)
  Working on and Submitting Programming Exercises (4 min)
* TODO VI. Logistic Regression (Week 3)
  Classification (8 min)
  Hypothesis Representation (7 min)
  Decision Boundary (15 min)
  Cost Function (11 min)
  Simplified Cost Function and Gradient Descent (10 min)
  Advanced Optimization (14 min)
  Multiclass Classification: One-vs-all (6 min)
* TODO VII. Regularization (Week 3)
  The Problem of Overfitting (10 min)
  Cost Function (10 min)
  Regularized Linear Regression (11 min)
  Regularized Logistic Regression (9 min)
* TODO VIII. Neural Networks: Representation (Week 4)
  Non-linear Hypotheses (10 min)
  Neurons and the Brain (8 min)
  Model Representation I (12 min)
  Model Representation II (12 min)
  Examples and Intuitions I (7 min)
  Examples and Intuitions II (10 min)
  Multiclass Classification (4 min)
* TODO IX. Neural Networks: Learning (Week 5)
  Cost Function (7 min)
  Backpropagation Algorithm (12 min)
  Backpropagation Intuition (13 min)
  Implementation Note: Unrolling Parameters (8 min)
  Gradient Checking (12 min)
  Random Initialization (7 min)
  Putting It Together (14 min)
  Autonomous Driving (7 min)
* TODO X. Advice for Applying Machine Learning (Week 6)
  Deciding What to Try Next (6 min)
  Evaluating a Hypothesis (8 min)
  Model Selection and Train/Validation/Test Sets (12 min)
  Diagnosing Bias vs. Variance (8 min)
  Regularization and Bias/Variance (11 min)
  Learning Curves (12 min)
  Deciding What to Do Next Revisited (7 min)
* TODO XI. Machine Learning System Design (Week 6)
  Prioritizing What to Work On (10 min)
  Error Analysis (13 min)
  Error Metrics for Skewed Classes (12 min)
  Trading Off Precision and Recall (14 min)
  Data For Machine Learning (11 min)
* TODO XII. Support Vector Machines (Week 7)
  Optimization Objective (15 min)
  Large Margin Intuition (11 min)
  Mathematics Behind Large Margin Classification (Optional) (20 min)
  Kernels I (16 min)
  Kernels II (16 min)
  Using An SVM (21 min)
* TODO XIII. Clustering (Week 8)
  Unsupervised Learning: Introduction (3 min)
  K-Means Algorithm (13 min)
  Optimization Objective (7 min)
  Random Initialization (8 min)
  Choosing the Number of Clusters (8 min)
* TODO XIV. Dimensionality Reduction (Week 8)
  Motivation I: Data Compression (10 min)
  Motivation II: Visualization (6 min)
  Principal Component Analysis Problem Formulation (9 min)
  Principal Component Analysis Algorithm (15 min)
  Choosing the Number of Principal Components (11 min)
  Reconstruction from Compressed Representation (4 min)
  Advice for Applying PCA (13 min)
* TODO XV. Anomaly Detection (Week 9)
  Problem Motivation (8 min)
  Gaussian Distribution (10 min)
  Algorithm (12 min)
  Developing and Evaluating an Anomaly Detection System (13 min)
  Anomaly Detection vs. Supervised Learning (8 min)
  Choosing What Features to Use (12 min)
  Multivariate Gaussian Distribution (Optional) (14 min)
  Anomaly Detection using the Multivariate Gaussian Distribution (Optional) (14 min)
* TODO XVI. Recommender Systems (Week 9)
  Problem Formulation (8 min)
  Content Based Recommendations (15 min)
  Collaborative Filtering (10 min)
  Collaborative Filtering Algorithm (9 min)
  Vectorization: Low Rank Matrix Factorization (8 min)
  Implementational Detail: Mean Normalization (9 min)
* UNDERGOING XVII. Large Scale Machine Learning (Week 10)
  Learning With Large Datasets (6 min)
  Stochastic Gradient Descent (13 min)
  Mini-Batch Gradient Descent (6 min)
  Stochastic Gradient Descent Convergence (12 min)
  Online Learning (13 min)
  Map Reduce and Data Parallelism (14 min)
* TODO XVIII. Application Example: Photo OCR
  Problem Description and Pipeline (7 min)
  Sliding Windows (15 min)
  Getting Lots of Data and Artificial Data (16 min)
  Ceiling Analysis: What Part of the Pipeline to Work on Next (14 min)
* TODO XIX. Conclusion
  Summary and Thank You (5 min)
